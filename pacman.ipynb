{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN for Ms Pacman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TLDR:** This notebook provides an implementation of a reinforcement learning agent using a deep q network to play Ms. Pac-Man.\n",
    "<br>\n",
    "<br>\n",
    "$\\rightarrow$ a detailed description is provided in [README.md](README.md) or [Project Documentation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Environment](#environment)\n",
    "2. [Training](#train-agent)\n",
    "3. [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import Video\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.DQN import DQN\n",
    "from src.ReplayBuffer import ReplayBuffer\n",
    "from src.utils import preprocess_state\n",
    "\n",
    "# for windows\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for mac\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "np.random.seed(187)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"ALE/MsPacman-v5\"\n",
    "env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Observation Space: \", env.observation_space)\n",
    "print(\"Action Space: \", env.action_space)\n",
    "print(\"Actions: \", env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample states\n",
    "obs, info = env.reset()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 10), nrows=1, ncols=5)\n",
    "for i in range(5):\n",
    "    obs, reward, done, trunc, info = env.step(env.action_space.sample())\n",
    "    obs = preprocess_state(obs, stack_states=False, create_tensor=False)\n",
    "\n",
    "    axes[i].imshow(obs)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 1000\n",
    "LEARNING_RATE = 1e-4\n",
    "REPLAY_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 100\n",
    "N_EPISODES = 1000\n",
    "N_STACKED_FRAMES = 4\n",
    "INPUT_SHAPE = (4, 84, 84)\n",
    "\n",
    "# create experience tuple template\n",
    "Experience = collections.namedtuple(\n",
    "    \"Exoeruebce\", field_names=[\"state\", \"action\", \"reward\", \"done\", \"next_state\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "dqn = DQN(INPUT_SHAPE, env.action_space.n).to(device)\n",
    "target_net = DQN(INPUT_SHAPE, env.action_space.n).to(device)\n",
    "target_net.load_state_dict(dqn.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(capacity=REPLAY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = EPSILON_START\n",
    "frame_idx = 0\n",
    "episode_rewards = list()\n",
    "episode_durations = list()\n",
    "\n",
    "for episode in tqdm(range(1, N_EPISODES + 1), total=N_EPISODES, desc=\"Training\"):\n",
    "    start = time.time()\n",
    "    state, info = env.reset()\n",
    "    state = preprocess_state(\n",
    "        state, stack_states=True, stack_size=N_STACKED_FRAMES, create_tensor=True\n",
    "    ).to(device)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "        # use epsilon greedy strategy to select action\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            action = dqn(state).argmax().item()\n",
    "\n",
    "        # act in environment\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        # preprocess state\n",
    "        next_state = preprocess_state(\n",
    "            next_state,\n",
    "            stack_states=True,\n",
    "            stack_size=N_STACKED_FRAMES,\n",
    "            create_tensor=True,\n",
    "        ).to(device)\n",
    "        total_reward += reward\n",
    "\n",
    "        # store information in replay buffer\n",
    "        exp = Experience(state, action, reward, done, next_state)\n",
    "        replay_buffer.push(exp)\n",
    "        state = next_state\n",
    "\n",
    "        # restore experiences from replay buffer\n",
    "        if replay_buffer.size > BATCH_SIZE:\n",
    "            (\n",
    "                sample_states,\n",
    "                sample_actions,\n",
    "                sample_rewards,\n",
    "                sample_dones,\n",
    "                sample_next_states,\n",
    "            ) = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # format samples as tensors\n",
    "            sample_states = torch.cat(sample_states).to(device)\n",
    "            sample_actions = torch.LongTensor(sample_actions).to(device)\n",
    "            sample_rewards = torch.FloatTensor(sample_rewards).to(device)\n",
    "            sample_next_states = torch.cat(sample_next_states).to(device)\n",
    "            sample_dones = torch.FloatTensor(sample_dones).to(device)\n",
    "\n",
    "            # calculate q values\n",
    "            q_curr_state = (\n",
    "                dqn(sample_states).gather(1, sample_actions.unsqueeze(-1)).squeeze(1)\n",
    "            )\n",
    "            q_next_state = target_net(sample_next_states).max(dim=1)[0]\n",
    "            expected_q = sample_rewards + GAMMA * q_next_state * sample_dones\n",
    "\n",
    "            # calculate loss\n",
    "            loss = nn.MSELoss()(q_curr_state, expected_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(dqn.state_dict())\n",
    "\n",
    "        # update epsilon\n",
    "        epsilon = max(EPSILON_END, EPSILON_START - frame_idx / EPSILON_DECAY)\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_durations.append(time.time() - start)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save model\n",
    "torch.save(dqn.state_dict(), \"agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation environment\n",
    "eval_env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "# uncomment the following line to record a video for each episode\n",
    "eval_env = gym.wrappers.RecordVideo(eval_env, \"videos\", episode_trigger=lambda x: True)\n",
    "\n",
    "# uncomment the following line to load the model from saved file\n",
    "# dqn = DQN(INPUT_SHAPE, eval_env.action_space.n).to(device)\n",
    "# dqn.load_state_dict(torch.load(\"dqn.pth\"))\n",
    "dqn.eval()\n",
    "\n",
    "# evaluation loop\n",
    "eval_episode_rewards = list()\n",
    "eval_episode_durations = list()\n",
    "\n",
    "for episode in tqdm(range(1, N_EPISODES), total=N_EPISODES, desc=\"Evaluation\"):\n",
    "    start = time.time()\n",
    "    state, info = eval_env.reset()\n",
    "    state = preprocess_state(\n",
    "        state, stack_states=True, stack_size=N_STACKED_FRAMES, create_tensor=True\n",
    "    ).to(device)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # since we are evaluating the model, we use greedy strategy\n",
    "        action = dqn(state).argmax().item()\n",
    "        next_state, reward, done, _, info = eval_env.step(action)\n",
    "        next_state = preprocess_state(\n",
    "            next_state,\n",
    "            stack_states=True,\n",
    "            stack_size=N_STACKED_FRAMES,\n",
    "            create_tensor=True,\n",
    "        ).to(device)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    eval_episode_rewards.append(total_reward)\n",
    "    eval_episode_durations.append(time.time() - start)\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10, 5), nrows=2, ncols=2)\n",
    "axes.flatten()\n",
    "axes[0].plot(episode_rewards)\n",
    "axes[0].set_title(\"Training Episode Rewards\")\n",
    "\n",
    "axes[1].plot(episode_durations)\n",
    "axes[1].set_title(\"Training Episode Durations\")\n",
    "\n",
    "axes[2].plot(eval_episode_rewards)\n",
    "axes[2].set_title(\"Evaluation Episode Rewards\")\n",
    "\n",
    "axes[3].plot(eval_episode_durations)\n",
    "axes[3].set_title(\"Evaluation Episode Durations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ve_rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
